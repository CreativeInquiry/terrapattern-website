## How it Works

Terrapattern uses a deep convolutional neural network (DCNN), based on the [ResNet](http://arxiv.org/abs/1512.03385) ("Residual Network") architecture developed by Kaiming He et al. We trained a 34-layer DCNN using hundreds of thousands of satellite images labeled in [OpenStreetMap](https://www.openstreetmap.org/), teaching the neural network to predict the category of a place from a satellite photo. In the process, our network learned which high-level visual features (and combinations of those features) are important for the classification of satellite imagery. 

It has been [pointed out](http://www.spacemachine.net/views/2016/3/datasets-over-algorithms) that many machine-learning breakthroughs are constrained, not by the limitations of algorithms, but by the availability of high-quality training datasets. The Terrapattern project illustrates this situation well. When we began the project, we expected that we could find a pre-existing DCNN model for satellite imagery, such as those hosted at the [Caffe Model Zoo](http://caffe.berkeleyvision.org/model_zoo.html). We settled in for a much longer haul when we realized we would have to train our own. To that end, Terrapattern was only possible due to the astonishing crowdsourced mapping effort of the OpenStreetMap project, which has generously categorized large parts of the world with its [Nominatim](http://wiki.openstreetmap.org/wiki/Nominatim/Special_Phrases/EN) taxonomy. We trained our DCNN using 466 of the Nominatim categories (such as "airport", "marsh", "gas station", "prison", "monument", "church", etc.), with approximately 1000 satellite images per category. Our resulting model, which took 5 days to compute on an nVidia 980 GPU, has a top-5 error rate of 25.4%.

After training the model, we removed the final classification layer of the network and extracted the next-to-last layer of the DCNN. Using this layer of proto-features (a technique called "[transfer learning](https://www.tensorflow.org/versions/r0.8/tutorials/image_recognition/index.html)"), we computed descriptions for millions more satellite photos that cover the metropolitan regions of New York, San Francisco, and Pittsburgh. When we want to discover places that look similar to your query, we just have to find places whose descriptions are similar to those of the tile you selected. To perform this search in near real time, we use the [CoverTree algorithm](https://github.com/manzilzaheer/CoverTree) for [K-Nearest Neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).

The Terrapattern search tool features three visualizations: a slippy map, for specifying visual queries; an "Geographical Plot" (or minmap), which shows the locations of search responses in the surrounding metro region; and a "Similarity Plot", which organizes the returned results within an abstract 2D space using [Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis), or PCA. The Terrapattern website is built using Ruby and JavaScript, with satellite imagery from Google Maps, while the Geographical PLot and Similarity Plot were created in JavaScript using [p5.js](http://p5js.org/). 

For more technical information on Terrapattern, including our open-source code, models and datasets, please see this [list of technical references](http://www.terrapattern.com/references#technical-bibliography) and [our Github repository](http://github.com/CreativeInquiry/terrapattern).